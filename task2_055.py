# -*- coding: utf-8 -*-
"""task2_055.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1N7byY8F4QLcVQcbqkGPm0flv92p6Okjl

<div class="alert alert-block alert-danger">

# **FIT5196 Task 2 - Assessment 1**
    
#### Student Name: Alexandra Goh & Sothearith Tith
#### Student ID: 29796431 & 27208001

Date: 19th April 2024


Environment: Python 3.10.12

Libraries used:
* os
* pandas 1.1.0
* multiprocessing
* itertools
* nltk 3.5
* nltk.tokenize
* nltk.stem
* langdetect
* collections

</div>

<div class="alert alert-block alert-info">
    
## **Table of Contents**

</div>

[1. Introduction](#Intro) <br>
[2. Importing Libraries](#libs) <br>
[3. Examining and Loading Input File](#examine) <br>
$\;\;\;\;$[3.1. Read CSV file](#read) <br>
[4. Text Extraction and Cleaning](#text-extract) <br>
$\;\;\;\;$[4.1. Extract strings from CSV file into dictionary](#extract-dict) <br>
$\;\;\;\;$[4.2. Check for duplicate records](#duplicate) <br>
$\;\;\;\;$[4.3. Remove links and urls from 'textOriginal' field](#remove-links) <br>
$\;\;\;\;$[4.4. Remove emojis from 'textOriginal' field](#remove-emojis) <br>
[5. Count English Comments](#english-comments) <br>
$\;\;\;\;$[5.1. Detect English comments](#detect-english) <br>
$\;\;\;\;$[5.2. Tokenize comments](#tokenize) <br>
$\;\;\;\;$[5.3. Count number of English comments](#count-english) <br>
$\;\;\;\;$[5.4. Write to a CSV file](#write-csv) <br>
[6. Loading and Parsing Comments](#load-comments) <br>
$\;\;\;\;$[6.1. Get channels with number of English comments greater than 15](#eng-15) <br>
$\;\;\;\;$[6.2. Removing context-independent stopwords](#context-independent) <br>
$\;\;\;\;$[6.3. Analyzing the frequency of words across documents after removing stopwords](#freq-words) <br>
$\;\;\;\;$[6.4. Removing context-dependent stopwords](#context-dependent) <br>
$\;\;\;\;$[6.5. Removing rare tokens](#rare-tokens) <br>
$\;\;\;\;$[6.6. Stemming Tokens](#stem-tokens) <br>
$\;\;\;\;$[6.7. Removing tokens with length less than 3](#less-3) <br>
$\;\;\;\;$[6.8. Getting all unique unigrams](#unigrams) <br>
[7. Generating Bigrams](#GeneratingBigrams) <br>
$\;\;\;\;$[7.1. Getting Top 200 bigrams](#200-bigrams) <br>
$\;\;\;\;$[7.2. Ensuring collocations can be collocated within the same comment](#collocations) <br>
$\;\;\;\;$[7.3. Combining unigrams and bigrams in a single list](#combine-unigrams-bigrams) <br>
$\;\;\;\;$[7.4. Calculate the vocabulary containing both unigrams and bigrams](#calc-vocab) <br>
$\;\;\;\;$[7.5. Generate token index](#token-index) <br>
$\;\;\;\;$[7.6. Write output as vocab.txt](#vocab-txt) <br>
[8. Generating Sparse Numerical Representation](#sparse) <br>
$\;\;\;\;$[8.1. MWE Tokenizer](#mwe) <br>
$\;\;\;\;$[8.2. Write output as countvec.txt](#countvec-txt) <br>
[9. Summary](#summary) <br>
[10. References](#Ref) <br>

<div class="alert alert-block alert-warning">

## **1.  Introduction**  <a class="anchor" name="Intro"></a>
    
</div>

The foundational step in any NLP endeavor is the conversion of text into numerical values, allowing machines to comprehend and discern patterns within the language. This process is not only iterative but also critical in defining the features that will inform your machine learning model or algorithm.

We will be developing Python code to preprocess a collection of YouTube comments, sourced from an Excel file, and transform them into numerical representations.

These representations are crucial as they constitute the standard format for text data, rendering them compatible for integration into various Natural Language Processing (NLP) systems such as: recommender-systems, information-retrieval algorithms, machine-translation etc.

<div class="alert alert-block alert-warning">

## **2.  Importing Libraries**  <a class="anchor" name="libs"></a>
    
</div>

Any python packages is permitted to be used. The following packages were used to accomplish the related tasks:

Environment: Python 3.10.12

Libraries used:
* os (for interacting with the operating system, included in Python xxxx)
* pandas 1.1.0 (for dataframe, installed and imported)
* multiprocessing (for performing processes on multi cores, included in Python 3.6.9 package)
* itertools (for performing operations on iterables)
* nltk 3.5 (Natural Language Toolkit, installed and imported)
* nltk.tokenize (for tokenization, installed and imported)
* nltk.stem (for stemming the tokens, installed and imported)
* langdetect (for detecting language of the given text)
* collections (to implements specialized container datatypes)
"""

!pip install langdetect
!pip install langid

import os
import re
import langid
import pandas as pd
import multiprocessing
from itertools import chain
import nltk
from nltk.probability import *
from nltk.tokenize import RegexpTokenizer
from nltk.tokenize import MWETokenizer
from nltk.stem import PorterStemmer
from nltk.util import ngrams
import ast
from langdetect import detect, DetectorFactory
from nltk import word_tokenize
from collections import Counter

"""-------------------------------------

<div class="alert alert-block alert-warning">

## **3.  Examining and Loading Input File**  <a class="anchor" name="examine"></a>
    
</div>

The input file is an Excel file containing multiple worksheets with many YouTube comments data. Each Excel table consists of two columns:
* `id`: unique comment identifier
* `snippet`: a JSON array, in string format, that contains information about one top level comment for a particular YouTube video, such as the comment, the channel and author.

<br>

Our goal is to extract the ‘textOriginal’ fields in all top level comments for all YouTube videos listed. Here are some examples of ‘textOriginal’:
* `textOriginal`: 'Awesome bro love Ur way of teaching'

* `textOriginal`: 'https://youtube.com/shorts/rmMUmdBKBvU?si=dZeuo3mmpfLdnWZP.   English literature short notes on topic what is literature'

* `textOriginal`: 'help me please. Please please i need to help my mom\r\n.\r\n\r\n\r\n\r\nBinance pay ID : 346984027\r\n\r\nor\r\n\r\nUSDT trc20 address :TVq6tjrZdtj85xjRJooiAcuEGV8T1jbALR\r\n\r\nor \r\n\r\nBTC network address : 1D8bZwU6iBbefoHWdsn7nf4neFMQDzMbgE\r\n\r\nor \r\n\r\nBUSD BEP20 address : 0xa6e039d45c32a010d9b9afbc2d273a9e0ed2c915'

* `textOriginal`: '感想，上位期間，MV有浸浴缸浸過頭既鏡頭，唔應該著衫，完全無feel😅😅😅😅😅你話想表達果個情感又好，引起話題又好，都做唔倒，不如搵個肯唔著衫既做MV主角啦❤️'

<br>

Observing the variety in the `textOriginal` entries, we see that they are not exclusively in English and contain a diverse range of different elements including URLs, emojis, and numerical figures. This diversity highlights the complexity nature of the data thus, requiring sequential processing techniques to generate a vocabulary list and numerical representation for the corresponding text.

<div class="alert alert-block alert-warning">

### **3.1.  Read CSV file**  <a class="anchor" name="read"></a>
    
</div>
"""

# Connect to Google Drive to access the excel file

from google.colab import drive
drive.mount('/content/drive')

# Because the data are positioned differently in each worksheet, we have to search through each row and column unitl we reach the data.
# Function to find the header row index and header columns
def find_data_start(sheet_data):
    for row in range(sheet_data.shape[0]):
        for col in range(sheet_data.shape[1]):
            value = sheet_data.iat[row, col]
            if pd.notnull(value):  # Found a non-empty cell, potentially a header
                return row, col
    return None, None

# Function to parse the Excel file
def parse_excel(file_path):
    # Load the Excel file
    xls = pd.ExcelFile(file_path)

    # Dictionary to hold data from all sheets
    all_sheets_data = {}
    all_records = []

    # Iterate through each sheet
    for sheet_name in xls.sheet_names:
        # Load the current sheet into a DataFrame
        sheet_data = pd.read_excel(xls, sheet_name=sheet_name, header=None)

        # Find the starting point of data
        header_row, header_col = find_data_start(sheet_data)
        if header_row is not None and header_col is not None:
            # Adjust header row, since pandas uses zero-based indexing
            # Calculate the columns to use by creating a range from the starting column to the end
            usecols = range(header_col, sheet_data.shape[1])

            # Now read the sheet again with proper skiprows and usecols
            df = pd.read_excel(xls, sheet_name=sheet_name, skiprows=header_row, usecols=usecols, header=0)
            all_sheets_data[sheet_name] = df
            all_records.append(df)
    df1 = pd.concat(all_records)

    return df1

# Pass the locate of the excel file then extract all the data and store in dataframe
file_path = '/content/drive/Shareddrives/FIT5196_S1_2024/A1/Students data/Task 2/Group055.xlsx'
data = parse_excel(file_path)
print("Shape of the dataframe", data.shape)
data.head()

"""<div class="alert alert-block alert-warning">

## **4.  Text Extraction and Cleaning**  <a class="anchor" name="text-extract"></a>
    
</div>

<div class="alert alert-block alert-warning">

### **4.1.  Extract strings from CSV file into dictionary**  <a class="anchor" name="extract-dict"></a>
    
</div>

The extracted `snippet` field contains many elements that we do not want, moreover, it is in string format. To get the text in `textOriginal` fields, we will use python `literal_eval()` function to convert from string to dictionary which makes it easier to get the text we want.

We used `apply()` function to appy `extract_data` function to every of the dataframe to extract `channelId` and `textOriginal` and store them in the two newly created columns.

We also used `try` and `except` to catch any row that cannot be changed to dictionary due to syntactic errors.
"""

# get channelId and textOriginal
def extract_data(snippet):
    try:
      # convert snippet string to dictionary
      snippet_dict = ast.literal_eval(snippet)
      return [snippet_dict['topLevelComment']['snippet']['channelId'], snippet_dict['topLevelComment']['snippet']['textOriginal']]
    except Exception as e:
      # print(f"Error: {snippet}")
      # Any invalid rows will be mark for removal
      return ['TO_REMOVE','']

channelId_comment = data['snippet'].apply(extract_data)

data['channel_id'] = channelId_comment.apply(lambda x: x[0])
data['all_comments'] = channelId_comment.apply(lambda x: x[1])

# Display the new DataFrame
print("Shape of the dataframe", data.shape)
data.head()

"""<div class="alert alert-block alert-warning">

### **4.2.  Check for duplicate records**  <a class="anchor" name="duplicate"></a>
    
</div>

After extracting all the comments in textOriginal and its corrosponding channel_id, we need to make sure that the data is well-suited for preparing textual data for analysis by ensuring it is free of duplicates and missing identifiers.

<br>

This preprocessing step is critical in data science projects, especially when dealing with large datasets where such issues can skew results and hinder analysis. This approach not only enhances data quality but also facilitates more accurate and reliable outcomes in subsequent analyses or machine learning applications.

* `drop_duplicates(subset=['channel_id', 'all_comments'], keep='first')`: this function call removes duplicate rows based on unique combinations of `channel_id` and `all_comments`. The `keep='first'` parameter ensures that only the first occurrence of a duplicate entry is kept, while subsequent duplicates are removed.
* `.dropna(subset=['channel_id'])` removes any rows where the `channel_id` is `NaN`. This is essential for ensuring data integrity, especially if the `channel_id` is crucial for subsequent analyses or operations.
"""

# Remove duplicates based on 'channel_id' and 'all_comments'
remove_duplicate = data.drop_duplicates(subset=['id'], keep='first')
print("Number of duplicate rows removed:", len(remove_duplicate))

# Remove rows where 'channel_id' contains 'TO_REMOVE'
filtered_data = remove_duplicate[~remove_duplicate['channel_id'].str.contains('TO_REMOVE')]

# New dataframe containing only the columns that we need
channels_comments_data = filtered_data[['channel_id', 'all_comments']].copy()

print("Shape of the dataframe", channels_comments_data.shape)
channels_comments_data.head()

"""<div class="alert alert-block alert-warning">

### **4.3.  Remove links and urls from 'textOriginal' field**  <a class="anchor" name="remove-links"></a>
    
</div>

Some comments contain URls or HTML tags which need to be removed as they do not add any value to the text data. We used the `re` library, which provides regular expression matching operation to remove any HTML tags or URLS.

* Example of comments with URL: https://youtube.com/shorts/rmmumdbkbvu?si=dzeuo3mmpfldnwzp.   english literature short notes on topic what is literature

* Should be converted to:   english literature short notes on topic what is literature

<br>

``` r'<[^>]+>' ```
This regular expression will match any string that starts with an opening angle bracket `<`, followed by one or more characters that are not a closing angle bracket `>`, and ends with a closing angle bracket `>`. It is commonly used in parsing tasks to remove HTML tags from a text, as HTML tags are typically enclosed within angle brackets.

![image.png](data:image/png;base64,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)

<br>

``` r'http\S+|www\S+' ```
This regular expresion will match any string that starts with "http", followed by any non-whitespace characters, or starts with "www", followed by any non-whitespace characters. This effectively captures most URLs, whether they start with "http://" or "www".

![image.png](data:image/png;base64,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)

<br>

For each row in the dataframe, we used `apply()` function to pass the comment to `rmove_html_url()` function which will then return a cleaned comment.
"""

# Remove html tags, urls and links
def remove_html_url(snippet):
    """
    Remove HTML Tags and URLs.
    """
    try:
        # Remove HTML tags
        clean_html = re.sub(r'<[^>]+>', '', snippet)
        # Remove URLs
        clean_url = re.sub(r'http\S+|www\S+', '', clean_html)
        return clean_url
    except Exception as e:
        print(f"Error: {snippet}")
        return snippet

# Apply the function to the 'all_comments' column
channels_comments_data['all_comments'] = channels_comments_data['all_comments'].apply(remove_html_url)

"""<div class="alert alert-block alert-warning">

### **4.4.  Remove emojis from 'textOriginal' field**  <a class="anchor" name="remove-emojis"></a>
    
</div>

Some comments contain emojis which can introduce ambiguity and noise, especially in tasks that rely on clear textual content leading to inconsistencies. Removing emojis can standardize the data, ensuring that the text is uniformly processed.

<br>

* Example of comment with emoji:
感想，上位期間，mv有浸浴缸浸過頭既鏡頭，唔應該著衫，完全無feel😅😅😅😅😅你話想表達果個情感又好，引起話題又好，都做唔倒，不如搵個肯唔著衫既做mv主角啦❤️

* Should be converted to:
感想，上位期間，mv有浸浴缸浸過頭既鏡頭，唔應該著衫，完全無feel你話想表達果個情感又好，引起話題又好，都做唔倒，不如搵個肯唔著衫既做mv主角啦

<br>

``` emoji_pattern = '|'.join([re.escape(s) for s in emoji_content]) ```
constructs a regular expression pattern to match emojis. `emoji_content` is a list or collection of emoji characters.

<br>

``` re.escape(s) ``` ensures that each emoji character in `emoji_content` is treated as a literal string in the regular expression, escaping any special characters that might be part of the emoji.

<br>

``` '|'.join([...]) ``` combines each escaped emoji into a single string with a vertical bar '|' between them, which represents the logical OR operator in regular expressions. This means the pattern will match any single emoji from the `emoji_content` list.

<br>

``` data['all_comments'] = data['all_comments'].apply(remove_emojis) ``` applies the `remove_emojis` function to each element in the `all_comments` column of the `data` dataframe. The apply() method iterates over each entry in the column, passing it to `remove_emojis` function to clean the text by stripping out emoji characters.
"""

# Read emoji from emoji.txt
emoji_file_path = '/content/drive/Shareddrives/FIT5196_S1_2024/A1/emoji.txt'

with open(emoji_file_path, 'r') as emoji:
        # Read the content of the file
        emoji_content = emoji.read()

print("Number of emojis:", len(emoji_content))

# Remove emoji
emoji_pattern = '|'.join([re.escape(s) for s in emoji_content])

def remove_emojis(snippet):
    """
    Remove emojis from the 'textOriginal' field of the snippet dictionary.
    """
    try:
      cleaned_text = re.sub(emoji_pattern, '', snippet)
      return cleaned_text
    except Exception as e:
      print(f"Error: {snippet}")
      return ''

# Apply the function to the 'snippet' column
channels_comments_data['all_comments'] = channels_comments_data['all_comments'].apply(remove_emojis)

"""<div class="alert alert-block alert-success">

## **5.  Count English Comments**  <a class="anchor" name="english-comments"></a>
    
</div>

<div class="alert alert-block alert-warning">

### **5.1. Detect English comments**  <a class="anchor" name="detect-english"></a>
    
</div>

After cleaning all the comments, we are now going to label each comment on whether the comment is in English or not.


``` if detect(snippet) == 'en': ``` If the detected language is English (denoted by 'en'), the function returns [1,1]. The first element in the returned list (1) indicates that the snippet is in English, and the second element (1) acts as a counter to keep track of the total number of snippets processed. If the detected language is not English, the function returns [0,1]. Here, 0 indicates the snippet is not in English, and 1 again seems to act as a total count.

<br>

``` data[['eng_comment_count', 'all_comment_count']] = data['all_comments'].apply(is_english).tolist() ``` The apply() method processes each snippet in `all_comments`, and the `is_english` function returns a list for each snippet.

<br>

``` .tolist() ``` converts the series of lists returned by apply() into a list of lists.

<br>

Finally, this list of lists is used to create two new columns in the dataframe: `eng_comment_count` and `all_comment_count`. The former stores the value indicating whether the comment is in English (1 for yes, 0 for no), and the latter to be a constant count (always 1), serving as a way to keep track of the total number of comments processed.
"""

DetectorFactory.seed = 0

def is_english(snippet):
  try:
    # Detects the language of the text
    if detect(snippet) == 'en':
      #[eng,total]
      return [1,1]
    return [0,1]
  except Exception as e:
      # print(f"Error: {snippet}")
      return [0,1]

# Apply the function and split the results into two columns
channels_comments_data[['eng_comment_count', 'all_comment_count']] = channels_comments_data['all_comments'].apply(is_english).tolist()
print("The shape of channels_comments_data:", channels_comments_data.shape)
channels_comments_data.head()

"""<div class="alert alert-block alert-warning">

### **5.2. Tokenize comments**  <a class="anchor" name="tokenize"></a>
    
</div>

Now that we have do not have to worry about working with different languages, we can safely tokenize all the comments for further processing regardless of what languages they are in. This is because the regex pattern below will only capture comments that contain English alphabets.

<br>

* Example of untokenized comment: 'awesome bro love ur way of teaching'

* Will be converted to: [awesome, bro, love, ur, way, of, teaching]

<br>

The function below aims to tokenize the given snippet (a string of text) into individual words or tokens using RegexpTokenizer with the pattern ```r"[a-zA-Z]+"```, which matches sequences of one or more alphabetical characters, effectively ignoring numbers, punctuation, and special characters.  

![image.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAeYAAADECAYAAABKgIJGAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAADsMAAA7DAcdvqGQAACUgSURBVHhe7d0NWFVVvj/wL/IuICiC+C5aTvjSDbUUtUfFySYqu9hoY02TTjlTE0/3mfCOY1r2os30H6n7n9FrVpZNaTMxylMWjTa+MI5iNwm9kRqloKAiigIC8qbc/dtnoXA8wAHOyz6b76dnP2evfc5B2J2zv3utvfZaXg0aEBERkSF0U49ERERkAAxmIiIiA2EwExERGQiDmYiIyEAYzERERAbCYCYiIjIQBjMREZGBMJiJiIgMhMFMRERkIAxmIiIiA2EwExERGQiDmYiIyEAYzERERAbCYCYiIjIQBjMREZGBMJiJiIgMhMFMRERkIAxmIiIiA2EwkxuUIu+z1UiaMxFDvbzgpS19R0/H/b9ajfTvS9VrPEvp/tV4LG6o/rd49X0Mm4vUE0RE7cRgJtc6twNL42MwNGE5dvhMxJLN27F9x3asfjoWAbuX4+4bY3D3a9ladHuQ0nQsvTcJ65CAt9K1v2djMuKj1HNERO3k1aBR60TOVZGJFXdMxNL8BKzcsgHJ48LUE41KkflSAiY+l4m4l7/CjsVaWKtnDO1ACsbELkT0n49h08PRaiMRUcewxkwuk73mSSzdF4slqZtshLIIQ9yzG7Dh4ShkPvMkVh9Qm42uuhrZ8ugTqBeJiDqDwUyuUb0Dm17V4uuOJ/Ho5NbqwdF48OmFiEWmVqvOVNug1aS94DVnI/JKD2Pjb+7GiL5ybbovRsxciI05thu+i3auRtLMEeirX8ceiok/X4HNR6rVs3aoyEP6Hx7D9NF9LdeOb5yIx17ajMNX/7kibJyjbY9bqpc2P2h53f0f8AIzEXUcg5lcIycT67S8ir1rjBa9bbhlIhK0h6LNey010Ualn2LpXfFYV5qA5Ru349ONSYg5nIKHRicgpVntuhqZv5uO2PgkpAc8hBS57rs5GSPyV+H+mHis2GdHOOvN7kNx96t5iP5ZCj7dsR2bnhqBvP++HyPu0n5uvrwoDPG/0X72fz+qvyX+2U369fKlt1taA/STCS2oV+zTi0RE9pFrzETOdnrjLOnL0DBr42m1pTWnGzbMhvb6Bxs2qZfvfVHKaIh7cW/DJcsmi+yVDbHa9thXv1IbNNq2ONn2rNVrG45Zfm7UkobtzZ+wcqlh++Io7d+La1ieafXCw6satJOGhqgnPm24oDY1ZC63+bc1/s7LM9UGIiI7sMZMBlaN6nq1qovHo3PjmncIU7Xr7IprteDMLSuRiVlY+Aur12p19VkLkrWq+Aqk726l1ly6A5t/p1Xv/3M5kidYNbvf9CiS/1P7EWs2YEcbLdZxzzbIiS+WTFAbiIjswGAmAwtAgI9a1cWgb2+12oTe5er0BXWLVRHyvpbE7IkL3+3Ajp3Nl72F8lO0IM8/rb/apu+zsUl7mDU62kav8ACMiJ2lPW7EYb05m4jIsRjM5BJRN46B3NqbV9RKIF6Vh7zd2kPUCEQ3ux+4L3ra6swtzpVq9WuhvTdVHtchKX46plsvP0/BYe3ZHUWtVHe1Wrr+LHtZE5EbMJjJNUbF4VEtZLM//0qLzjYc2ItNWjJGzY/Ta7ftE4XoRO3hlpX4qsHSlGxzeTbO8nJbtFq6fj5Qf0kvEhG5EoOZXCMgHvf/RgvDz5Zi9eet9YrOw8ZXVyIbcVg4J95GU3Jb+iLqBu1BC/fDhZYtTVV/vlAf/nPhllbGFrshFvdrD5szD9sYgawah7I3a48PImaIZQsRkSMxmMllYhekYPmEIqT87H6k7LcVjDLy10N46L0ixL2YgidvUZvbJQATZy1HLDZj5Rs7mgdrfTZWP5eCopxoTIxrqU1cExaPWYu1OvOa5VhtfWvVkXVI+YP2OP9+DrtJRE7BYCbXCY7Dks++wspbv8LCW2Mw4sGFWJdm6ZS1+Z2FeGh0DCY+l4f4l/ci/VnrHtX2C5iQjNUvxiH7pekYM2cFNn6m/Rtpq/HYjAQs3BeFBzcuwSwbnciuCUD8M5u1k4hMLE28G4/9YSPS5Xf802OYPi0J6Tc8ik3PzUIr0a7jfcxE1BEMZnKtsFgkf3wYx9KXIr5+L1bMsnTKevLVbATetRzbC/KwfXFcm6HXugDEPbsXp3esQkL1BiQnaP+GVoveG3w/Vu0+jA1z7RjPWk4iPj+ETb+KRt6fk3G3/I5v5CH6V5tw6Mu3MIvN2ETkJJzEgoiIyEBYYyYiIjIQBjMREZGBMJiJiIgMhNeYyaUqK8tQXFyA8vLzqKurUVuJ2uKFgIBAhIVFoE+fwfD19VfbicyHwUwuU1SUj8LC71SJqGN8fHwRHT0KoaGt3vNG5LHYlE0uUVJyiqFMDlFfX4ejRw/i0qVKtYXIXBjM5BInTx5Va0Sdd+XKFZw+fUyViMyFwUxOV1Z2DrW1rY2PTdR+588XaQF9WZWIzIPBTE5XXc0mR3IOfrbIjBjM5HTsX0jOws8WmRGDmYiIyEAYzERERAbCYCYiIjIQDjBCTtfZgUX8gsLgqy3e/oHw8jLmuWTDlcuor65EbcV5/dHdvP0C4RfSC74BwfDy9lFbjcNR+ysm5jYEBYWqEpE5sMZMhta990AEhveHT0CQYUNZeHXzhm/3HgiKHIKA0Ei11T38Q8IRHDVUP6ExYigLI+0vIqNhMJNhdQ8foB+8PY1/aIQeju7gq9UeA3pGqZJncOf+IjIiBjMZkk9AsB4ynso/LFKrFbr+6+WptU937S8iI+I3gQzJE2vKTUmzu2+ga/8Gae7v5uOnSp7FHfuLyKgYzGRI3r4Bas1zdXPx1ISu/vcczdN/fyJHYTCTMXmpR0/m5do/wsvTd5qL9xeRUTGYiYiIDITBTEREZCAMZiIiIgNhMBMRERkIg5mIiMhAGMxEREQGwmAmIiIyEAYzERGRgTCYiYiIDITBTEREZCAMZjKBSzi1OxX//LLEUqw4igNpHyG3zFLsiPIvU5H+3ttIT8tEOUrwTZq2rpV3Nf4bJnDh4EfYtTMXdVKoP43c9FR8la8/1W5dYX8RuQqDmTxeVc42HDgVjMExMqdvKXJ3ZuBsyHAM4GRFLSvMwP6vaxERMwy+WrFo73Z8XzsAgwdYniYi92Ewk8crKSqB/5CxGBwspWJcKA1Bv9gYdO/EnAg9bp2NqTeHAQEhCEQ4Ribep//8wDBzTOhffroYdX1GYWSUt1YqQUlJLcJHxiHcx/J8e5l9fxG5EoOZTKEm95MmzaYXcTw9Fd90phW1Ih+5x0rh3zNcr1FWHTuIooow9Iy0PG0KRZlI/0euKmjxvO9tfPGdKrRXV9hfRC7CYKauozgLW1YvQVJCHIb36W9ZJsxE0qL1yMgrVy+yqDl+GCUht2H8+L5aqRSFR84geEw8hodanje7c2kLru2j1pZXs/TXd/X9ReRIXg0atU7kFEVF+SgsbF9VLDhqKLz9AlWp82qy/oifJbyCbClEj0XcoABtpRoFGVkolG0Yi19rtewnxjpusv6aiyWovlCkSs7nHxKOgJ5RqtQ557a9iOS3clTJSkUBMrNOaCuRuPfNrUiZ6ZhqcUf2V0zMbQgKYvqTuTCYyencHsw1e/DauDlYUzwKP3/vPfx2RpMgqT+BbUsWIGm9FkKz38TeVQnorZ7qLE8O5padwJak+5CcWozYRR/jz0+PhaNOZRjMRBZsyibzy8nBgeE3ImLuf+BXTUNZ+AzCjMfnIU7WU79Hgb6RWlL4/lI9lDF2EZY96bhQJqJrGMxkfmN/iXc37cKe/0qAzTuo/AMQolbd5et1M3DrrTaWxTtglDuB5XJAcvJ2IDIRKWufwgimMpFTMJip66ksR3nxd8jek47UV5Yg6bH/h23qKWpBQRqemSfX6Mfi1+tTcO9AtZ2IHI7XmMnpjND5C2XfYdu6tVj34QfIzlPblIjoQUDeCZzFIvz1zFOIVds7yzTXmCuzsGb2TLyWFYl7X/8IKYna/nICXmMmsmAw26Giphq19fVo0P5r1ImxKzqlW7duCPT1g7+P3C3qGdwezFeDRQvhmyZh6vTpGDH6RgwbNBA33DAIvSvTkRSbpNWaGczXc15nL2sM5muqamtQU1+HK00Oz+465nh5eSFAO+bIcYdcg8HcirJLVSipLMflK1fUFuMI9g9ARHAofLxl5CZjc3cwH1p7J/79uRxEzF6Fv65KxHWjThakIWkcg/l6Nch+dTYeeCWr5X3nQAxmSyCfqyjXQ9loAnx90Vs75jCgnY/XmFtwvrICxRdLDRnKQmrxhaUlqL98WW0h24pxdL/lftzY6ZNsBkthRhqvMdtQmJaMJC2UpbPXsmecG8pk+U6f1L7TRgxlUV1Xh8IL51BZW622kLMwmG24VFer15SNru5yPYorOjGFUpcQiQExlluktqWn42ilvmpRX45DaUssPY2pGb0H9uNpOBs5Se/sNaOfeoKcQpqspSLgCYovljW5qEfOwKZsG4rKL+Bi9SVVMr6BPSP0Ziajcvs15oI0JCckYUuxFAYhdspABFwd9SsScU/PR8y2V/B2TiJWfb0K1rc6d5TnNmXnYM3oO/Ga7K+ro6S1JAGLPpyHEarUGV25Kbu0qhJnPegkOzIkFKGBQapEjsYasw1ynceTXKrzrN/X5QYmImX7h1g2bzqGRZ5AdsYeZJ4ARsxbgTf3ZeDdRb/E1CnywjRk7Dd+S4nzaZ8n/SRGk5eFTNlfLS7l8mrqpCoP+w5X1daqNXIG1pht+K74lFrzDD27B6N3sHEnHzbE7VJu4Pmdv1yrK9eYCy6cQ3Wd54RdoJ8/BnBKT6dhjZmIiMhAGMxEREQGwmAmIiIyEAYzERGRgTCYiYiIDMTjgjk3Nxcvv/wyZsyYgX79+sHHx0dfZF22yXPyGiIiMqbdu3dj4cKFiIuLQ69evfQ5APz9/TFs2DD8+Mc/xtq1a1FW1nUHT/KY26UkbF944QVs3LhRbWndgw8+iGXLlmH48OFqi/14u5Rjdeh2qT5D4e3v4bdLlZ9DdekZVXI+v5BeCOzZV5U8T0f2F2+Xco+O3i61detWrFixQg/mtvj5+eHpp5/GSy+9pFe+uhKPqDG/+eabGDFihN2hLOS18h55L3mey3WePx7vFRcPGuHqf8/RPP33p9ZJDflHP/qRXaEsamtr8fvf/x6jR4+2+z1mYfhglqbpX/ziF7jcgcka5D3yXvkZ5Fnqqjx7BK6Ghiuou+Tav6G+uhJX6j1zRCZ37C9ynblz5yIlJUWV2ufIkSOYMmUKPvnkE7XF/AzdlC21XQlWa1F9euOm4dHo368PeoRYxmstv1iJk6fO4EhuHorOnNO3NfXGG29gwYIFqtQ6NmU7VkeaskX38AHw9dBmShnBSkaycjXZX7LfPE1H9xebst2jPU3ZTzzxBF5//XVVumZY9EDcOGywfjzv3j1Qr0idv1CGEwWn8M2Ro6ioqFKvtJBr0Hv37sWYMWPUFvMybI1ZrinL/1BrU2+/FfffdwdGxtyAsNAQvdOALLIu2+Q5eY01+VlO6RR26SgOpG3GN+qYUnUsA7vSs1Chl4qxZUF/DO/TH0lpMvhwFtZo61Jek6W/AOVfpiL9vbeRnpaJcpTgmzRtXSvv+tL1B3WjqSop9Miac03ZWbeEsqirLHPpMKCO4M795Zku4dTuVPyz8RhRIcegj5Cr+kqdS1ugH2OGL0iDVFGyX7Ucc4a/qg46JZnYJcec91L141bzY5Bjvf/++9eFcqh2rJ559zT86I7JGDZ0oHZiFQgvL8DHxxuREb0wbswoPPLgfbjl5pvUOyxqamrw5JNPqpK5GTaYpaOXdfP1vQlT9fBti7xGXtuU/Cz5mY51Ed/vzMDZkBhE99KKZVnYv7cAgTcMR7DlBdRJVecKcKnkpN5MK82dRtVw5bJ+ElFZnI/qssYZINxDQq6i6BhqK0vRcLlebTUWI+0vT1OVsw0HTgVjcIzUWEuRqx+DhmOAARvNpANuU+G9wpB4z3QM7N/2mO6TJsRi4vhbVMli3759WLVqlSqZlyGbsqVm+4Mf/ECVLKQWbE8oN/XN4e+xa/eXqmTx7bffttlT2/6m7Fx88V4Wgu+ai5G9teJ3nyE9JwyTE+Ng+Y5IjTkWyR8DM17PxqrEAq3GPBOvac/8Ov0knhirvwhVBzdj16nhuOOuUfDVa80foWLUzzH+RsvzbTFrUzZRW7piU3bBP95Gbsg9mD5e5ieVY9BBBCfMxkjVsiw15omPpwMzV2Hvm4ko0GrMD7yiPbHoY+Q+rQ46FVn4Z9ppDHzgHkT7WWrN/yr7NyT80L67WOxpyn777bfx6KOPqpLF7MQ79Vpxe+zI+AKHvz2mStCzQa47m5kha8x/+9vf1JqFXINobygLeY+8tynrn915l3D8M0uTkK7iMP51tUkoAD3UZ3BALzl4RGBAgpRGwd9fHjUV+cg9Vgr/nuFaKEtT+EEUVYShp4PmBCYi86nJ/aTJ5a6LOJ5+7RjkF6QOHgMjEaI9RAzUDzoYEaQOOg0XUZR9DBWBYeihhbI0hX9//CKCwx170ElNTVVrFtI03d5QFuPH3azWLKRyZfZe2oYM5l27dqk1C+no1VHW77X+2c7VA8NGT9IeJ2HYIPlSDMIA/bLJOAwbKI/aF+z4YZSE3Ibx4+X+01IUHjmD4DHxGO75lQAicoMeN45CnPYYN3Qg5KgzYJClUjMmepD+iOrjOH4mGDE/nAyp81YczcGFsNsw5pYwy/MOsmPHDrVmEfODoWqtfeQa9PAbhqiSRUZGhlozJ0M2ZcsoXqdPn1Yl4KEH7tE7d3VEadlFbPhr+7rZ5545qdY6rybjRYyeA/z5xHOYoH1LClMXID41Hjs+nAtH9Z395qsDSLzrblUynhdeWIyEhGmqROQ4Tz21BJmZzS9XeaKc/KPwCwxQpU6q2YPfD9IOOh8ew2+naAedgjQkjfsAU/Z9iNkdr+M0c+B/9mPOvfepUtuCg7vrHbo66tCRo9j5z/9RJeAnP/kJPvjgA1UyH0PWmIuLm3cGabwlqiM6815H8J/ynBb0llAWA2a/iVwHhjIRUTP+k/BbrXKhh7IYmIhVZxwXyh0REty543CI1XH87Nmzas2cDNsrm4iIqCsyZDBHRjbvhCCDh3RUZ95LRESdd7Gic8fhi1bH8YiICLVmToYM5lGjRqk1CxnRq6Os33vHHXdALqu3tniayZMn2/w7HLlMndr8vnBrQ4YM0V8j9y3u3LlTX/Ly8vT32hq9jcgRPv/8c/1zJkvj504+g/Z8XuV11p9zdy09ehj3dkdb4iZPsvl3NF1kEopGMoqXjOrVUSdPNb+8OXLkSLVmToYMZusvlQyz2VHW723rC0vXmzZtWou92eXg1vTA+Pzzz+v7WBY5+BE5m3zOGk8MZZHPoHwWJRzkczlv3jz1ymvy8/P1AYfkteQc8fHxas2i6b3I7VFZeQm53+erkoWMnW1mhgxmmY+zKRn7WgYLaS95j/W42dY/m1rXUijLwU4OenJgYwCTUcln85133tE/q9ajUAmGs/PMnj1brVkc+N8jKD57XpXs98X+/1VrFjLAyO23365K5mTYSSweeuih66Z5lGE2Bw2wb77ZE4WnsSW9eaDIHM0bNmxQpZZxEgsLOWDZGsZUaiPtaXnoyMhfMhezf3Av+AQEw8vb+HOxytCX9TVVqK04rw8f6m7efoH6/Myy/7oZfP/JjFh1VRdRc/Fcu4cQbe/IX7Y+043h3Z7PtKOZdRKLYcOG4dixazVlGZLz3rum6vcm2yP74GHs/eKAKln86U9/QlJSkiqZk2GDWYbllPmUrcfLtmdoTltDcXp7e+PQoUNtDscpGMyWgViktmytvaEs2hvM/j16IyCsjyp5HpmUwZ3jP/uHhCOgZ9tjERuNjJ8tE5fUX7JMAWOPjgzJKc3Y0dHN7x2ScJZatbuYNZhlEouHH35YlSxkEospk8e1Ol62hNLefdl6LbupCRMmIDMzU5XMy1s7gzRkO054eDj69u2LLVu2qC0W+SdOoaCwSHUu8NUXWS8rr8DRvALs3puFnEPXN3uvXbsWM2bMUKXWna+8qNY8Q6CvH7prXxRHmj9/vn4Aa6ojoSwqKkpRXm5fE5ZfUBgCe9nXKmJUPgFBWjXwCi7XXlJbXEemfQzs1U+VPIuXVzf4dQ/V52WWkLZHRER/7RjQvoE5wsLC9CD+6KOP1BagtLRU33bLLc0nTXCV8uoq1Nv5NxuBr7cPegR0V6WW3Xzzzfq4FPv371dbtBPXmlp8+10+zp8v0/6fe8Hfzw++vr56JexcSSkOf3sU/9iZiRMF1waZEjLt4+bNm/VcMDvD1pgbvfzyy1iyZIkqdcyKFSvwzDPPqFLbunqN2VZtWa7PdfQcrj015pB+w9HNR0YN92wNWjBfPPmtdtLo2hmxQvrdqO2/a71hPVFdVRmqzhWqUus6M4mFdbO2O2vNZp6PWcydOxd/+ctfVKn9JMA//vhj3HPPPWqLuRl+gBEJ1DfeeENvim4veY+8tz2hTJYOMU3JAcsVDSs+gcGmCGXh1a0bfLq79hYY/Xqyh4ey8NVqzVJ7djbr3trSQrR+/XpVIkeS4TOTk5NVqX1uuukmfWzsrhLKwvDBLBYsWKBfH5bOW/aS18p75L3UPta9sG31ZnUGbx/HNse7m7eva/+ebi7+95zJFX+LnHBah7Otzo7kGCtXrsTf//53u3tUy33Qixcvxtdff236XtjWDN+UbU06hcnUjRIeOTk5V8fVltHCZGASuQYqt0TZ08mrJV25KVtqDHJ9uanOfkTsbcr29E5f1mrKS1BdWqRKzuffI1zbf57X6cuWijPHcLmm7Wv0nZ2P2VZHMHccEs3elG1Npm2Ua/x79uzRp3GUa/xynXnAgAGIjY3VB4KSiSpCQ7vmNHseF8yu0JWDWUK5aXOe1CjkVpLOYDC7BoO5Y6zv1e9oJ8fO6GrBTK3ziKZsch3rZmyzj7BDZP0Zt/4OkA2szzkVg5masb5Fyp2DLhC5gvVn3OyT8JPxMZjpKutQFtJBhsjMrD/jtr4HRK7EYKarWFumrojBTEbDYKarrA9IrC0TkU1eXmqFnIHBbIN3N8/aLZ72+xK5Q2s1YXtrzc6qTXvcMccFA8B0Zdy7Njh63GlnCzTRwBJEziA9reV+5ZZGsLMnmGWb/Azr+/wdobuHfYe7+3n+CHNGxmC2ITQwSK0ZX5B/AAJ8HTOMpfXBaPDgwWqNyHNJKDeO/S4je3VkeNnGUBa2BuHprB6B3T2m1uzj7a39vp5zjPREDGYbZLam8CDXjnPcETLDS2Rw1xwZh8he1iHa3nBuGsqNJJwdOa52Ny8vRIaEqZKxRYaEgleYnYvB3IJeQcH6F8WoZ7HBWk1ZRt6Rs9eu6sLBj7BrZy7qpFB/GrnpqfiqtUuA59OxpE9/DNeWSS/vQY3abEv5l6lIf+9tpKdlohwl+CZNW9fKu74sUa/wfK3vvyysUfvK3mVNluWdBf+w7Kv0f+RqpVx8Ieva8oX9U3I7lIzkZd1UbW842wplISPiWY+z3Vnyne6vfaf9DTqRi7TMyTEnqJ3TbFL7MZhbERrYHUN7R6FvaC+9Bh0eFOKWpVeTJUI7Wx3UK0L/nbpyKKMwA/u/rkVEzDDIYaxo73Z8XzsAgwdYnralcOtfkIpIREQCZ/9/GjLsmyLanDqw/zyVhHJHwrm1UO7sMLUtkf4t8v2WgJZhdm0dD1yxND3myO8hgTywZ4Q+FCc5H4PZDnImKzXoph9WVy5NvzBhgUGGPaN2pfLTxajrMwojo+TkpAQlJbUIHxmHcB/L89c7gcy07dpjAn46f5L2+AFSt57Qn7Glx62zMfXmMK2aEIJAhGNk4n0YHAwEagcoM2h7/43FE2dOIre15dgurJitneVoImavwr1j9VUM/OHPMWawN3y7h2il4Rj/8GRtD4YguJfleXdobzi7I5SbkoCWMfBtHQ9csTQ95sjvwUB2LQYzea6iTNVcalGyr5Xm0pzPsEFGWpw5DrMTEyGjI2e89RkO6U/aUJGvBU8p/HuG6zXKqmMHUVQRhp6WHDKH9uy/65zAlkVzsCS1WMvwRVj1SiIaK9tXLhxE3snLCNb2HXAZ5V8dRolPOHq6MZhFa+FsPT62rc5drgplIgYzdQE12Pfx63oIz0iYhN7RkzBjulbIeR1b99i+0lxzXAuTkNswfnxfrVSKwiNnEDwmHsPZ105Tg+xXk5AsoRyZiJS1TyG2SSfdC9/moi56BsbE+AENBTh27DIG3h6HfgboMdRSOLeFoUyuxGkf6Spp0pPaQ6Nly5a1q/dqS9w+7WPNHrw2bg7WFM/FqsMrMUOruZWnL8S4+R8Ac9/E/v9KgDP64Jt12seajBcRP2ctziIS9765FSkzHd+M4OxpH6WpWm6hkse2MJTJ1VhjJtMr356mhbK2Mi8BE1Rzao9JCfiprHzwF2zN0zeRPQrS8EyShDIQu+gtvOyEUHYFe2vODGVyBwYzmdwJbH1fqxlrtbsnZk66VjMOnYQZ/yGhsh0b0nMs26h1lVlY88skbNFOcqSzV8rTY+HJXYLaCmeGMrkLg5nMLW8Ptkln7Mj5mDqpaYz4Y8LMxzFCWzv0+qfY19pNzS7w9boZuPVWG8viHTDGndPS2esxvCb3Klt19vJkLYUzQ5ncicFMpnYofT2kM/aIx+MRa9l0zahJmDlKeyz+I7ZsL7dsIxta7+zl6SSUrYP5kUceUWtErsdgJvOq2YOtr1uaqQ+9eKeN0aruxO9VK3bq+5+i0LLqFqMf3YYvv7Sx/C4e7r5zujAtGUmv6FVl/Hp9Cu4daNlORM7BYCbTqtn/L/xNOn1Fj0XclEktLGMtTbLb12MbLzVfp+bgH5H8eJqlB/brq/DEWA40QeRsDGYyqXJkpP5R7z08Y9FbePfDD1tY3sdv58rrc7Du49bHz+5yKrPw9uJXkK2t6j2wEwdZthORUzGYyZzyPkWqdMbGXMyc0totPT0wZfZTiNDWuvz42c1c6+xlhh7YRJ6EwUymVLjvM73TF+bGX713uSX+4ybjx3p2tz5+dtfRpLOXiXpgE3kKBjOZUA62vSX3SAE/TZjc9qhe/pNw5+PSPbuN8bO7jBzs0zt7abJewQNDrTvN2V6S0uSCPhF1FoOZzCdrB9ZJR67IpzBjin2DbY5ImIc4WWll/GwiIldgMJP5jH0Ke2Rawq8XYYK9F0aj5+JdfTrDbPy62UAkXZEdUz7aWFYleubwnERGw2AmIiIyEAYzERGRgTCYiYiIDITBTOQ0nOqciNqPwUxERGQgDGYiIiIDYTCTcTSw6bczTLX7+FGgLozBTIZxud5cA3tcrnPt33PFxf+eM5npbyFqLwYzGUb9pQpcqa9TJc/WcOUK6qvKVck16qtl/9WqkueqqyrTav9XVImo62Ewk6HUlJljvGX5O9wRLtWevv8aGrS/QSbrJOq6GMxkKLWVpaguPaNKnqlGC5aaiyWq5Fp1lWWovlCkSp6l4cplVJ47wWZs6vIYzGQ4NeXnUHHmmBYypWi4XK+2Gpv8nnVV5agsznd7rVVOCiqKjuknOVc8YP9J83tNeQkunv5ev5xB1NV5NWjUOnVxzz//PF544QVVApYtW6Zv66yionwUFn6nSkSOExNzG4KCQlWp46ZNm4Zdu3apErBz505MnTpVlYhcizVmIiIiA2EwExERGQiDmYiIyEAYzOR0Xl5eao3IsfjZIjNiMJPTBQQEqTUix+Jni8yIwUxOFxraG35+AapE5Bi9ekWhWzdvVSIyDwYzuUT//sPUGlHndevWDX37DlUlInNhMJNLhIf3w4ABN6oSUcf5+Phi2LB/Q2Agm7HJnBjM5DJRUUP0ASHCw/vC19dfbSWyhxcCArprn6HBGDkyTr88QmRWHPmLrnLWyF9ERhcdHY38/HxVAvLy8jBkyBBVInIt1pjpKusD0fHjx9UaERG5CoOZrrIO5qY1CCIzs/6ss7ZM7sRgJiIiMhAGM13FGjMRkfsxmKlFEswMZzK79evXqzULTvdI7sZgpqukxmx9UGIwk9llZGSoNQteXyZ3YzBTM1OmTFFrFk0njycyI+vPuPV3gMjVGMzUjHWN+d1331VrROZk3SrEpmxyNwYzNWOrA5j1NTgis5g/f75au4ZN2eRuHPmLrjNt2rRmzXtyoJKRkIjMxno+53feeQfz5s1TJSL3YI2ZriNDcTYltWYOzUlmIyeg1hjKZAQMZrqOXGOzda2ZHcHILOSzbP15ZiiTUbApm2ySg5Z1jUKatHfu3MlrcOTRpAVIJq2wxokryChYYyabpMZsq0lbwprN2uSppCOjrVDmCScZCYOZWiRNe9ZN2hLOMjWkhLOsE3kC+axKD2xbvbDlBJS3SJGRsCmbWtVYS7YVwlLDkAPaI488wgMbGVLjiWRLt/zJ51Zqy0RGwmCmNsnBTQ5scoBrSWNIDx48+GpIyzY2D5KzNZ40Nj5K/wgZZrOtzopSU+ZlGTIiBjPZTQ5irYUzkSeQk0W5X5mtPGRUDGZqF3tqz0RGJIEsl11YSyajYzBThzQGtD1NhkTu0ng5RQKZ9ymTp2AwU6dZX9tres2vcZ3IWRr7MTQ+Ns4OJU3VbK4mT8RgJiIiMhDex0xERGQgDGYiIiIDYTATEREZCIOZiIjIQBjMREREBsJgJiIiMhAGMxERkYEwmImIiAyEwUxERGQgDGYiIiIDYTATEREZBvB/FCiHyE8qDyUAAAAASUVORK5CYII=)

<br>

``` tokenizer.tokenize(snippet) ``` tokenizes the snippet based on the defined regular expression, splitting the text into tokens consisting only of alphabetic characters and returns a list of word tokens from the snippet.



``` data['word_tokenization'] = data['all_comments'].apply(tokenized_text) ```
applies the `tokenized_text` function directly to each element in the `all_comments` column of the data dataframe.
The results are stored in a new column `word_tokenization` in the same dataframe containing a list of tokens derived from the corresponding text in `all_comments` coulmn.
"""

def tokenized_text(snippet):
  try:
      tokenizer = RegexpTokenizer(r"[a-zA-Z]+")
      return tokenizer.tokenize(snippet.lower())
  except Exception as e:
      print(f"Error: {snippet}")
      return ['']

# Apply the function and split the results into two columns
channels_comments_data['word_tokenization'] = channels_comments_data['all_comments'].apply(tokenized_text)

# data['word_tokenization'] = word_tokenization.apply(lambda x: x)
print("Shape of channels_comments_data:", channels_comments_data.shape)
channels_comments_data.head()

"""<div class="alert alert-block alert-warning">

### **5.3. Count number of English comments**  <a class="anchor" name="count-english"></a>
    
</div>

To count the number of English comments, we aggregate the data in the dataframe by the `channel_id` column and then sum up the numerical columns (`eng_commnet_count` and `all_comment_couunt`) for each unique `channel_id`.

<br>

``` data.groupby('channel_id') ``` this function groups the dataframe by the unique values in the channel_id column. The `groupby` method is commonly used in pandas to split the data into groups based on some criteria, in this case, the `channel_id`.

<br>

``` .sum() ``` the sum method is applied to each group. This method adds up all the numerical values in each group for each numerical column in the dataframe. It's important to note that the summation will only apply to columns with numerical data; non-numeric columns(`word_tokenization`) will be handled differently .ie., join all arrays together.

<br>

The result of the grouping and summation is then assigned to a new variable called `grouped_data`.
"""

grouped_data = channels_comments_data.groupby('channel_id').sum()
print("Shape of grouped data:", grouped_data.shape)
grouped_data.head()

"""<div class="alert alert-block alert-success">

### **5.4. Write to a CSV file**  <a class="anchor" name="write-csv"></a>
    
</div>

After we have counted the number of comments per channel, we can then save the output to a .csv file.

<br>

 `reset_index()` function is used to reset the index of the DataFrame, making it a regular column(s) again. This is necessary after grouping if you need to perform further DataFrame operations.

 <br>

``` .iloc[:, [0,5,4]]  ``` The iloc function is used to select specific columns or rows in a DataFrame by position. Here, it selects all rows (:) and the first (`channel_id`), sixth (`eng_comment_count`), and fifth (`all_comment_count`) columns.

<br>

``` to_csv('055_channel_list.csv', index=False) ```
writes the DataFrame `grouped_data_reset` to a CSV file named 055_channel_list.csv. The `index=False` parameter means that the DataFrame’s index will not be written to the CSV file. If index were `True`, each row would start with its index number in the CSV.
"""

# Write the DataFrame to a CSV file
grouped_data_csv = grouped_data.reset_index().iloc[:, [0,3,2]]
grouped_data_csv.to_csv('055_channel_list.csv', index=False)

# If you want to download the file to your local system from Google Colab, you can use the files module
from google.colab import files
files.download('055_channel_list.csv')

"""<div class="alert alert-block alert-success">

## **6.  Loading and Parsing Comments**  <a class="anchor" name="load-comments"></a>
    
</div>

<div class="alert alert-block alert-success">

### **6.1. Get channels with number of English comments greater than 15**  <a class="anchor" name="eng-15"></a>
    
</div>

Now we need to filter out channels that have less than 15 comments. As the `grouped_data` dataframe already grouped the data by the unique values in the channel_id column, we filter the dataframe by only selecting the row, representing one channel, that has values greater than 15 in their `eng_comment_count` column and store it in the `filtered_data` dataframe.

<br>

`.iloc[:, [0,3,4,5,6]]` this iloc function is used to create a new dataframe `grouped_data_reset` by selecting only `channel_id`, `all_comment`, `word_tokenization`, `eng_comment_count` and `all_comment_count` coulmns for further processing such as getting all channels that have more than 15 English comments.
"""

grouped_data_reset = grouped_data.reset_index()

# Filter the grouped DataFrame where 'eng_comment_count' is greater than or equal to 15
filtered_data = grouped_data_reset[grouped_data_reset['eng_comment_count'] >= 15].copy()
unfiltered_data = grouped_data_reset[grouped_data_reset['eng_comment_count'] >= 15].copy()

# Display the first few rows of the filtered data
print("Shape of filtered_data:", filtered_data.shape)
filtered_data.head()

# Store each channel and its tokens into a dictionary for MWE
unfiltered_dict = {row['channel_id']: row['word_tokenization'] for index, row in unfiltered_data.iterrows()}

print("Number of documents:", len(unfiltered_dict))

# Get all the words from the unfiltered_dict dictionary and store into a list
# to use for generating bigrams
unfiltered_words = list(chain.from_iterable(unfiltered_dict.values()))

print("Number of words:", len(unfiltered_words))

"""<div class="alert alert-block alert-success">

### **6.2. Removing context-independent stopwords**  <a class="anchor" name="context-independent"></a>
    
</div>

First of all, we need to remove context-independent stopwords (e.g. 'the', 'is', 'we') which is a common preprocessing step in text analysis and natural language processing (NLP) as it helps reduce the dataset size and focuses the analysis on words that carry more meaning.

<br>

By removing common words that do not contribute much information (stopwords), the remaining words are likely more relevant to the thematic substance of the texts. This can improve the accuracy and effectiveness of subsequent analysis like topic modeling, sentiment analysis, or keyword extraction.

<br>

The `f.read().splitlines()` reads the entire file content into a single string and then splits it into a list where each line becomes an element in the list. This way, each word in the stopwords file becomes a separate element in the stopwords list.

<br>

`stopwordsSet = set(stopwords)` then converts the list of stopwords into a set. Sets in Python provide a faster membership testing than lists, which is crucial here since the code checks membership for each word in the documents against the stopwords.

<br>

`remove_stopwords` function uses list comprehension to create a new list of words by filtering out any word in the token list that is a stopword.

<br>

The `apply()` method is then used to apply the `remove_stopwords` function to each element of the `word_tokenization` column of the `filtered_data` DataFrame.
"""

# Read stopwords from stopwords file
stopwords_file_path = '/content/drive/Shareddrives/FIT5196_S1_2024/A1/stopwords_en.txt'

stopwords = []
with open(stopwords_file_path, 'r', encoding='utf-8') as f:
    stopwords = f.read().splitlines()

# Remove stopwords
stopwordsSet = set(stopwords)

def remove_stopwords(token):
  return [w for w in token if w not in stopwordsSet]

filtered_data['word_tokenization'] = filtered_data['word_tokenization'].apply(remove_stopwords)

filtered_data.head()

"""<div class="alert alert-block alert-success">

### **6.3. Analyzing the frequency of words across documents after removing stopwords**  <a class="anchor" name="freq-words"></a>
    
</div>

Even though we removed all the tokens that were in the stopwords list, there may be some tokens that are not in the list and occur frequently across all channels. Here, we will try to find find those tokens.

<br>

`[set(tokenlist) for tokenlist in to_filter_dict.values()]` uses list comprehension combined with set to ensure that each token is counted once per channel (as sets eliminate duplicates within a channel).

<br>

`chain.from_iterable` is then used to flatten the list of sets into a single list, where each token appears as many times as it is present in different tokens.

<br>

`FreqDist()` is a constructor from the Natural Language Toolkit (nltk) that creates a frequency distribution. A frequency distribution is essentially a count of how often each token appears in the dataset. Here, `words_count1` (which contains all unique words from each channel, aggregated) is used to compute this frequency distribution.
"""

# counting number of documents the word is in after removing stopwords
words_count1 = list(chain.from_iterable([set(row['word_tokenization']) for index, row in filtered_data.iterrows()]))

# returns a list of the top 5 most common words along with their counts
word_freq1 = FreqDist(words_count1)
print("Number of unique tokens: ", len(word_freq1))
print("Number of channel: ", len(filtered_data))
print("99% of channel", len(filtered_data)*0.99, "\n")
word_freq1.plot(15, cumulative=False)
print("\nTop 5 most frequent tokens:", word_freq1.most_common(5))

"""<div class="alert alert-block alert-success">

### **6.4. Removing context-dependent stopwords**  <a class="anchor" name="context-dependent"></a>
    
</div>

By knowing which tokens are present in nearly all documents, one might consider these as potential candidates for an extended stopword list, especially if these tokens are not meaningful for analysis. The purpose of this code is to identify tokens that are extremely common across the document corpus.

<br>

We have 1300 documents and 99% of documents is 1287 so any tokens that appear more 1287 times will be considered as context-dependent and will be removed. However, from the frequency distribution graph above that shows the top 15 most common tokens, we can see that 'love' appears the most at ~900 times, which means that there are no tokens that can be considered as context-dependent stopwords.

<br>

`word_freq.most_common()` method from the FreqDist object returns all tokens sorted by their frequency in descending order. Each element in the output is a tuple where `v` is the token and `k` is the number of channels in which the token appears.

<br>

The function, `remove_ninety_nine_percent_words`, is designed to take a list of tokens and return a new list that excludes any tokens found in the `ninety_nine_percent_words` set.
"""

# 99% words
ninety_nine_percent = [word for v,k in word_freq1.most_common() if k >= len(filtered_data)*0.99]
print("99% toekns:")
print(ninety_nine_percent)

# Remove 99% words
ninety_nine_percent_words = set(ninety_nine_percent)

def remove_ninety_nine_percent_words(token):
  return [w for w in token if w not in ninety_nine_percent_words]

filtered_data['word_tokenization'] = filtered_data['word_tokenization'].apply(remove_ninety_nine_percent_words)

filtered_data.head()

"""<div class="alert alert-block alert-success">

### **6.5. Removing rare tokens**  <a class="anchor" name="rare-tokens"></a>
    
</div>

After removing context-dependent stopwords that appear frequently in 99% of the channels, we also need to consider rare tokens that appear in less than 1% of the total number of channels.

<br>

Tokens that are too rare may not provide enough information or could be outliers, such as typos or very uncommon proper nouns. By eliminating infrequently occurring tokens, the overall dataset size is reduced, which can optimize processing speed and improve the performance of NLP models that might be negatively impacted by too sparse data.

<br>

`word_freq.most_common()` function returns all word-frequency pairs from the frequency distribution `word_freq`, where each token (`v`) is paired with the number of documents it appears in (`k`). The tokens are then sorted by frequency in descending order.
"""

# counting number of documents the word is in after stemming
words_count2 = list(chain.from_iterable([set(row['word_tokenization']) for index, row in filtered_data.iterrows()]))

# returns a list of words along with their counts
word_freq2 = FreqDist(words_count2)
print("Number of unique words: ", len(word_freq2))
print("Number of doc: ", len(filtered_data))
print("1% of doc", len(filtered_data)*0.01, "\n")

ffd = FreqDist(word_freq2.values())
from pylab import *
y = [0]*14
for k, v in ffd.items():
     if k <= 10:
        y[k-1] = v
     elif k >10 and k <= 50:
        y[10] =  y[10] + v
     elif k >50 and k <= 100:
        y[11] =  y[11] + v
     elif k > 100 and k <= 500:
        y[12] =  y[12] + v
     else:
        y[13] =  y[13] + v
x = range(1, 15) # generate integer from 1 to 14
ytks =list(map(str, range(1, 11))) # covert a integer list to a string list
ytks.append('10-50')
ytks.append('51-100')
ytks.append('101-500')
ytks.append('>500')
barh(x,y, align='center')
yticks(x, ytks)
xlabel('Number of Words')
ylabel('Word Frequency')
grid(True)

"""As we can see from the word frequency graph above, there are over 30,000 tokens that appear only once and over 5000 tokens which appear twice. As we set the threshold for rare tokens at 1%, any token that appear less than 13 times will therefore be removed.

<br>

`one_percent_words = set(one_percent)` converts the list of rare tokens into a set for faster membership testing, which is essential in the next step where each token in each channel is checked against this set.

<br>

The `remove_ninety_nine_percent_words` function is defined to filter out any tokens from the input tokens that is present in `ninety_nine_percent_words`. This function is then applied to each list of tokens in the `word_tokenization` column of `filtered_data`. The result is a new list of tokens for each document, excluding tokens that appear in 99% or more of the documents.
"""

# 1% words
one_percent = [v for v,k in word_freq2.most_common() if k <= len(filtered_data)*0.01]
print("1% tokens:")
print(one_percent)

# Remove 1% words
one_percent_words = set(one_percent)

def remove_one_percent_words(token):
  return [w for w in token if w not in one_percent_words]

filtered_data['word_tokenization'] = filtered_data['word_tokenization'].apply(remove_one_percent_words)

filtered_data.head()

"""<div class="alert alert-block alert-success">

### **6.6. Stemming Tokens**  <a class="anchor" name="stem-tokens"></a>
    
</div>

In a corpus with multiple documents, stopwords and rare tokens may skew the consistency of word frequencies across documents. By removing them first, stemming can be applied uniformly across the corpus where we convert various forms of a word to a single base form using `PorterStemmer`. After removing stopwords and rare words, the remaining text is more likely to represent the key themes and content. Stemming this refined set of words ensures that the variations of a word (like "run", "runs", "running") are conflated to a single base form.

<br>

The `PorterStemmer` is an algorithm for stemming English words, and it's widely used due to its simplicity and efficiency. The statement `stemmer = PorterStemmer()` initializes a Porter stemmer instance from the NLTK library.

<br>

The `stemming()` function takes a list of words (token) as input and uses a list comprehension to apply the `stem()` method of the `Porter Stemmer` to each word.

<br>

The `apply()` method is used to apply the `stemming` function to each element of the `word_tokenization` column in the `filtered_data` dataframe. This operation updates each list of tokens with their stemmed versions.
"""

# Stemming tokens using Porter stemmer
stemmer = PorterStemmer()

def stemming(token):
  return [stemmer.stem(word) for word in token]

filtered_data['word_tokenization'] = filtered_data['word_tokenization'].apply(stemming)

filtered_data.head()

"""<div class="alert alert-block alert-success">

### **6.7. Removing tokens with length less than 3**  <a class="anchor" name="less-3"></a>
    
</div>

`remove_short_words` is designed to filter out tokens that are considered too short to be meaningful for the analysis. Short tokens often include conjunctions, prepositions, and other less informative types of words, though they can sometimes include important words like "no" and "up".

<br>

Moreover, after stemming, some tokens' lengths can become less than 3 and lose their meaning. For example, row 4 in the table above of the `filtered_data` has `pas` then `pa` after stemming.
"""

# Removing all tokens with lenght < 3

def remove_short_words(token):
  return [word for word in token if len(word) >= 3]

filtered_data['word_tokenization'] = filtered_data['word_tokenization'].apply(remove_short_words)

filtered_data.head()

"""<div class="alert alert-block alert-success">

### **6.8. Getting all unique unigrams**  <a class="anchor" name="unigrams"></a>
    
</div>

We then extract a list of unigrams (single words) across all channels.
"""

# concatenate all the tokenized patents(cleaned) using the chain.frome_iterable function
unigrams = list(chain.from_iterable([set(row['word_tokenization']) for index, row in filtered_data.iterrows()]))
print("Total number of unigrams:", len(unigrams))

"""<div class="alert alert-block alert-success">

## **7.  Generating Bigrams**  <a class="anchor" name="GeneratingBigrams"></a>
    
</div>

<div class="alert alert-block alert-success">

### **7.1. Getting Top 200 bigrams**  <a class="anchor" name="200-bigrams"></a>
    
</div>

After getting unigrams, we now proceed to identify bigrams that are not only frequent but also have a strong association.

<br>

We are combining all tokenized comments into one list to analyze bigrams to gain an insight into the overall, global bigram relationships across the entire dataset or corpus. This is effective in identifying which word pairs are most commonly used across all texts, which can help in understanding common language usage or prevalent phrases in a corpus.

<br>

`bigram_measures = nltk.collocations.BigramAssocMeasures()` creates an instance of `BigramAssocMeasures`, which is a class that provides various statistical measures to evaluate the strength of association between word pairs (bigrams). These measures include likelihood ratios, pointwise mutual information (PMI), chi-squared, among others.

<br>

`bigram_finder = nltk.collocations.BigramCollocationFinder.from_words(unfiltered_words)` initializes a BigramCollocationFinder object for a list of words (`unfiltered_words`). This object is capable of finding bigrams within the provided list and calculating their frequencies.

<br>

`bigram_finder.apply_freq_filter(20)` filters out bigrams that appear fewer than 20 times in the entire dataset. Because PMI favors rare word pairs; hence, without the filters, it is more likely to see bigrams that include uncommon and possibly uninformative words ranking highly just because they don't often appear together or with other words.

<br>

`bigram_finder.apply_word_filter(lambda w: len(w) < 3)# or w.lower() in ignored_words)` filters out bigrams containing tokens that do not meet certain criteria. The lambda function `lambda w: len(w) < 3` specifies that words with less than three characters should be excluded.

<br>

`top_200_bigrams = bigram_finder.nbest(bigram_measures.pmi, 200) # Top-200 bigrams` selects the top 200 bigrams based on the pointwise mutual information (PMI) measure provided by bigram_measures.pmi. PMI is a measure of the association between two tokens, indicating how much more often they appear together than would be expected if they were statistically independent.
"""

bigram_measures = nltk.collocations.BigramAssocMeasures()
bigram_finder = nltk.collocations.BigramCollocationFinder.from_words(unfiltered_words)
bigram_finder.apply_freq_filter(20)
bigram_finder.apply_word_filter(lambda w: len(w) < 3)# or w.lower() in ignored_words)
top_200_bigrams = bigram_finder.nbest(bigram_measures.pmi, 200) # Top-200 bigrams
print("Number of bigrams:", len(top_200_bigrams))
print("List of top 200 bigrams")
print(top_200_bigrams)

"""<div class="alert alert-block alert-success">

### **7.2. Ensuring collocations can be collocated within the same comment**  <a class="anchor" name="collocations"></a>
    
</div>

The purpose of this code is to validate the presence of any of the Top 200 bigrams within any of the comments.

<br>

`zip(comment, comment[1:])` creates pairs of consecutive words (bigrams) from each `word_tokenization` row. The zip function pairs the first word with the second, the second with the third, and so on.

<br>

`set(zip(comment, comment[1:]))` converts these pairs into a set, eliminating any duplicate bigrams within the same comment.

<br>

`chain.from_iterable` flattens a list of sets (one set per comment) into a single set containing all unique bigrams across all comments.

<br>

`check_bigram()` function passes the entire column of tokenized text `(filtered_data['word_tokenization'])` and a list of top 200 bigrams to check against.
"""

# Set to hold bigrams that exist in the comments
validated_bigrams = set()

# Define the checking function
def check_bigram(tokenized_text_list, bigrams):
    # Generate a set of all bigrams for this list of tokenized comments
    all_bigrams_in_comment = set(chain.from_iterable(set(zip(comment, comment[1:])) for comment in tokenized_text_list))

    # Check each bigram in the top 200 bigrams if it is present in the set of all bigrams in the data
    for bigram in bigrams:
        if bigram in all_bigrams_in_comment:
            validated_bigrams.add(bigram)

# Execute the function
check_bigram(unfiltered_data['word_tokenization'], top_200_bigrams)

# Convert the set back to a list if necessary
validated_bigrams = list(validated_bigrams)
print("Number of bigram collocated within the same comment:", len(validated_bigrams))
print("List of bigram")
print(validated_bigrams)

"""<div class="alert alert-block alert-success">

### **7.3. Combining unigrams and bigrams in a single list**  <a class="anchor" name="combine-unigrams-bigrams"></a>
    
</div>

Now that we have both bigrams and unigrams, we need to combine them together to form a vocabulary list. This list is then sorted alphabetically in an ascending order.
"""

# Extend the vocabulary list with bigrams
# Each bigram is joined by an underscore to make it a single token
unigrams.extend(['_'.join(bigram) for bigram in validated_bigrams])

# Remove duplicates and sort
vocabulary = sorted(set(unigrams))

print("Vocabulary:")
print(vocabulary)

"""<div class="alert alert-block alert-success">

### **7.4. Calculate the vocabulary containing both unigrams and bigrams**  <a class="anchor" name="calc-vocab"></a>
    
</div>

After combining all bigrams and unique unigrams together, we can then calculate the number of vocabulary we have.
"""

# Count the number of vocabulary
print("Number of vocabulary containing both unigrams and bigrams:", len(vocabulary))

"""<div class="alert alert-block alert-success">

### **7.5. Generate token index**  <a class="anchor" name="token-index"></a>
    
</div>

After forming the vocabulary list, we need to map each token to a unique index. It is crucial for many NLP tasks where tokens (words, phrases, symbols, etc.) need to be represented as numerical values so that it can facilitate various operations and optimizations that are critical for processing text at scale.
"""

# Generate a token index: a dictionary mapping each token to a unique integer
token_index = {token: index for index, token in enumerate(vocabulary)}

print("List of token with their index:")
print(token_index)

"""<div class="alert alert-block alert-success">

### **7.6. Write output as vocab.txt**  <a class="anchor" name="vocab-txt"></a>
    
</div>
"""

# Write the vocabulary to a text file
with open('055_vocab.txt', 'w') as file:
    for word in token_index:
        file.write(str(word) + ':' + str(token_index[word]) + '\n')

# If you want to download the file to your local system from Google Colab, you can use the files module
from google.colab import files
files.download('055_vocab.txt')

"""<div class="alert alert-block alert-success">

## **8.  Generating  Sparse Numerical Representation**  <a class="anchor" name="sparse"></a>
    
</div>

<div class="alert alert-block alert-success">

### **8.1. MWE Tokenizer**  <a class="anchor" name="mwe"></a>
    
</div>

Lastly, we will use MWE Tokenizer to retokenize each channel and recognise bigrams so that we can generate sparse numerical representation, including bigrams.

<br>

`mwetokenizer = MWETokenizer(validated_bigrams)` initializes the MWE Tokenizer where the `MWETokenizer` is a tool from the NLTK library designed to tokenize text with the recognition of multi-word expressions (bigrams in this case). `validated_bigrams` consists of tuples representing significant two-word combinations. The tokenizer will treat these bigrams as single tokens when they appear in the text, which helps in preserving the semantic meaning of phrases that are split in typical single-word tokenization.

<br>

`colloc_patents = dict((channel, mwetokenizer.tokenize(tokens)) for channel, tokens in to_filter_dict.items())` creates a dictionary `colloc_patents` where each key is a document identifier (`channel`) and each value is the tokenized version of the document (`tokens`). The `to_filter_dict` is a dictionary containing document text where `mwetokenizer.tokenize()` function is applied to each document, recognizing and merging any multi-word expressions that match the entries in `validated_bigrams`.
"""

mwetokenizer = MWETokenizer(validated_bigrams)
colloc_patents =  dict((channel, mwetokenizer.tokenize(tokens)) for channel,tokens in unfiltered_dict.items())

"""<div class="alert alert-block alert-success">

### **8.2. Write output as countvec.txt**  <a class="anchor" name="countvec-txt"></a>
    
</div>

The final step involves writing a frequency distribution of tokens for different "channels" into a text file, formatting each channel's data into a specific structure that combines the channel identifier and token frequencies.
"""

# Write the countvec to a text file
with open('055_countvec.txt', 'w') as file:
  for channel in colloc_patents:
    token_frequencies  = FreqDist(colloc_patents[channel])
    pdict = {token_index[n]: token_frequencies[n] for n in token_frequencies if n in token_index}

    # Convert dictionary items to string and join them with a comma
    pdict_str = ', '.join(f"{key}:{value}" for key, value in pdict.items())
    file.write(str(channel) + "," + pdict_str + "\n")

# If you want to download the file to your local system from Google Colab, you can use the files module
from google.colab import files
files.download('055_countvec.txt')

"""<div class="alert alert-block alert-success">

## **9.  Summary**  <a class="anchor" name="summary"></a>
    
</div>

Overall, we have carried out text pre-processing in the following sequence:

1. Reading CSV files and extracting text.
2. Data cleaning: remove duplicates, urls, http tags, null values and emojis.
3. Counting English comments.
4. Tokenization: breaking down paragraphs or sentences into smaller components.
5. Stopword Removal: remove stopwords from the text.
6. Removing Frequent and Rare Tokens: filtering out rare tokens, which are words that appear very infrequently within the dataset.
7. Stemming: reduces words to their root form, thereby conflating derivationally related words with a common base form
8. Handling Bigrams: handling bigrams by identifying and validating common pairs of words.
9. Creation of sparse representation: mapping the generated token with the vocabs and counting the frequency for each channel
10. Saving sparese respresentation as text file, vocabulary list as text file and channel list as csv file.

<br>

In summary, we have created a text pre-processing pipeline that is comprehensive and addresses the typical challenges of preparing raw text for deeper NLP analysis and modeling. It involves cleaning, normalizing, reducing, and structuring text data into a form that highlights the most meaningful and discriminative elements of the text, thus making it suitable for sophisticated analysis and prediction tasks.

-------------------------------------

<div class="alert alert-block alert-success">

## **10.  References**  <a class="anchor" name="Ref"></a>
    
</div>

[1] Pandas dataframe.drop_duplicates(), https://www.geeksforgeeks.org/python-pandas-dataframe-drop_duplicates/, Accessed 03/04/2024.

## --------------------------------------------------------------------------------------------------------------------------

[My Workspace](https://colab.research.google.com/drive/1N7byY8F4QLcVQcbqkGPm0flv92p6Okjl?usp=sharing)
"""